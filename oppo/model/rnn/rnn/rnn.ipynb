{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#每次可以输出多个变量\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 6\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#中文字体\n",
    "import matplotlib\n",
    "matplotlib.use('qt4agg')\n",
    "#指定默认字体\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "#解决负号'-'显示为方块的问题\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "from input import *\n",
    "from model import Model\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        level = logging.INFO, \n",
    "        format = \"[%(asctime)s] %(message)s\",\n",
    "        datefmt = \"%Y-%m-%d %H:%M:%S\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expname = \"classification\"\n",
    "\n",
    "params = {\n",
    "    \"train_data\": \"./data/train.txt\",\n",
    "    \"predict_data\": \"./data/predict.txt\",\n",
    "#     \"train_data\": \"../../../feature/rnn_train.txt\",\n",
    "#     \"predict_data\": \"../../../feature/rnn_predict.txt\",\n",
    "\n",
    "    \"dictionary_path\": \"./output/dictionary\",\n",
    "    \"model_path\": \"./output/model\",\n",
    "    \"train_summary_path\": \"./output/summary\",\n",
    "    \"eval_summary_path\": \"./output/eval\",\n",
    "\n",
    "    \"train_epoch\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": False,\n",
    "    \"dictionary_cutoff\": 0,\n",
    "    \"num_targets\": 2,\n",
    "    \"word_embedding_dim\": 200,\n",
    "    \"letter_embedding_dim\": 200,\n",
    "    \"tag_embedding_dim\": 100,\n",
    "    \"is_use_pretrained_word_embedding\": True,\n",
    "    \"pretrained_emnedding_dir\": \"./w2v/dict_tencent\",\n",
    "    \"keep_prob\": 0.5,\n",
    "    \"cnn_layers\": {\n",
    "        \"filter_width\": 3,\n",
    "        \"output_channel\": 64,\n",
    "    },\n",
    "    \"gru_layers\": {\n",
    "        \"padding_len\": 10,\n",
    "        \"gru_size\": 64,\n",
    "        \"gru_layers\": 2,\n",
    "        \"attention_size\": 32,\n",
    "    },\n",
    "    \"dense_layers\": [\n",
    "        {\"hidden_units\": 2048},\n",
    "        {\"hidden_units\": 512},\n",
    "        {\"hidden_units\": 128},\n",
    "    ],\n",
    "\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 1,\n",
    "    \"lr_decay_steps\": 10,\n",
    "    \"clip\": 0.2,\n",
    "\n",
    "    \"steps_per_run\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_id:  {'tag': 1, 'words': 1, 'letters': 1}\n",
      "vocab_size:  {'tag': 22, 'words': 9356, 'letters': 2707}\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set = load_data(params[\"train_data\"])\n",
    "dictionary = Dictionary(params, train_set)\n",
    "dictionary.save(params[\"dictionary_path\"])\n",
    "pad_id = dictionary.pad_id()\n",
    "print (\"pad_id: \", pad_id)\n",
    "vocab_size = dictionary.vocab_size()\n",
    "print (\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dictionary.to_id(train_set)\n",
    "valid_set = dictionary.to_id(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_bm = BatchManager(train_set, 1, params, pad_id)\n",
    "valid_bm = BatchManager(valid_set, 1, params, pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(params, vocab_size, dictionary)\n",
    "merge_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(targets, preds):\n",
    "    preds = [int(i[1]>=0.5) for i in preds]\n",
    "    return f1_score(targets, preds), \"\"\n",
    "\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_path = os.path.join(params[\"train_summary_path\"], expname)\n",
    "os.system(\"rm -rf %s\" % summary_path)\n",
    "model_path = os.path.join(params[\"model_path\"], expname)\n",
    "os.system(\"rm -rf %s\" % model_path)\n",
    "eval_summary_path = os.path.join(params[\"eval_summary_path\"], expname)\n",
    "os.system(\"rm -rf %s\" % eval_summary_path)\n",
    "os.system(\"mkdir -p %s\" % eval_summary_path)\n",
    "\n",
    "train_writer = tf.summary.FileWriter(summary_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 2., 3.],\n",
       "        [2., 3., 4.]], dtype=float32), array([[2., 3., 4.],\n",
       "        [3., 4., 5.]], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([20., 38.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1,2,3],[2,3,4]], dtype=tf.float32) \n",
    "y = tf.constant([[2,3,4],[3,4,5]], dtype=tf.float32)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run([x, y])\n",
    "    print(x.shape, y.shape)\n",
    "    sess.run(tf.diag_part(tf.matmul(x, y, transpose_b=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-30 22:38:31] TRAIN 10 steps[0]: loss 2.2118  f1 0.3843\n",
      "[2018-10-30 22:38:51] TRAIN 20 steps[0]: loss 0.6659  f1 0.0103\n",
      "[2018-10-30 22:39:09] TRAIN 30 steps[0]: loss 0.6956  f1 0.1074\n",
      "[2018-10-30 22:39:13] TRAIN 32 steps[0]: loss 0.6891  f1 0.0000\n",
      "[2018-10-30 22:39:34] TRAIN 42 steps[1]: loss 0.6871  f1 0.0000\n",
      "[2018-10-30 22:39:54] TRAIN 52 steps[1]: loss 0.6687  f1 0.0000\n",
      "[2018-10-30 22:40:13] TRAIN 62 steps[1]: loss 0.6661  f1 0.0000\n",
      "[2018-10-30 22:40:17] TRAIN 64 steps[1]: loss 0.8626  f1 0.0000\n",
      "[2018-10-30 22:40:37] TRAIN 74 steps[2]: loss 0.6713  f1 0.1571\n",
      "[2018-10-30 22:40:57] TRAIN 84 steps[2]: loss 0.5723  f1 0.0000\n",
      "[2018-10-30 22:41:17] TRAIN 94 steps[2]: loss 0.5712  f1 0.2637\n",
      "[2018-10-30 22:41:20] TRAIN 96 steps[2]: loss 0.3578  f1 0.7500\n",
      "[2018-10-30 22:41:40] TRAIN 106 steps[3]: loss 0.6743  f1 0.4962\n",
      "[2018-10-30 22:42:00] TRAIN 116 steps[3]: loss 0.3770  f1 0.6582\n",
      "[2018-10-30 22:42:19] TRAIN 126 steps[3]: loss 0.6315  f1 0.5862\n",
      "[2018-10-30 22:42:23] TRAIN 128 steps[3]: loss 0.4366  f1 0.6875\n",
      "[2018-10-30 22:42:44] TRAIN 138 steps[4]: loss 0.5729  f1 0.5905\n",
      "[2018-10-30 22:43:05] TRAIN 148 steps[4]: loss 0.3078  f1 0.6730\n",
      "[2018-10-30 22:43:24] TRAIN 158 steps[4]: loss 0.3542  f1 0.8529\n",
      "[2018-10-30 22:43:28] TRAIN 160 steps[4]: loss 0.2475  f1 0.8000\n",
      "[2018-10-30 22:43:49] TRAIN 170 steps[5]: loss 0.3865  f1 0.8153\n",
      "[2018-10-30 22:44:09] TRAIN 180 steps[5]: loss 0.2274  f1 0.8857\n",
      "[2018-10-30 22:44:27] TRAIN 190 steps[5]: loss 0.1900  f1 0.9141\n",
      "[2018-10-30 22:44:31] TRAIN 192 steps[5]: loss 0.2069  f1 0.8276\n",
      "[2018-10-30 22:44:52] TRAIN 202 steps[6]: loss 0.1700  f1 0.9099\n",
      "[2018-10-30 22:45:12] TRAIN 212 steps[6]: loss 0.1377  f1 0.9299\n",
      "[2018-10-30 22:45:31] TRAIN 222 steps[6]: loss 0.1056  f1 0.9314\n",
      "[2018-10-30 22:45:35] TRAIN 224 steps[6]: loss 0.0957  f1 0.8846\n",
      "[2018-10-30 22:45:56] TRAIN 234 steps[7]: loss 0.1115  f1 0.9210\n",
      "[2018-10-30 22:46:16] TRAIN 244 steps[7]: loss 0.0817  f1 0.9406\n",
      "[2018-10-30 22:46:35] TRAIN 254 steps[7]: loss 0.0639  f1 0.9542\n",
      "[2018-10-30 22:46:39] TRAIN 256 steps[7]: loss 0.1198  f1 0.8750\n",
      "[2018-10-30 22:46:59] TRAIN 266 steps[8]: loss 0.0953  f1 0.9425\n",
      "[2018-10-30 22:47:20] TRAIN 276 steps[8]: loss 0.0856  f1 0.9502\n",
      "[2018-10-30 22:47:39] TRAIN 286 steps[8]: loss 0.0674  f1 0.9519\n",
      "[2018-10-30 22:47:41] TRAIN 288 steps[8]: loss 0.0931  f1 0.9032\n",
      "[2018-10-30 22:48:01] TRAIN 298 steps[9]: loss 0.0752  f1 0.9561\n",
      "[2018-10-30 22:48:21] TRAIN 308 steps[9]: loss 0.0730  f1 0.9596\n",
      "[2018-10-30 22:48:41] TRAIN 318 steps[9]: loss 0.0625  f1 0.9590\n",
      "[2018-10-30 22:48:45] TRAIN 320 steps[9]: loss 0.0726  f1 0.9206\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "    steps_per_run = params[\"steps_per_run\"]\n",
    "    global_step = 0\n",
    "    best_f1 = 0.\n",
    "    start_time = time.time()\n",
    "    valid_step = 0\n",
    "    for epoch in range(params[\"train_epoch\"]):\n",
    "        train_bm.init()\n",
    "        while True:\n",
    "            global_step, loss, n_steps, (f1, _) = model.train(sess, train_bm, steps_per_run, metrics, \n",
    "                    merge_summary=merge_summary, train_writer=train_writer)\n",
    "            logging.info(\"TRAIN %d steps[%d]: loss %.4f  f1 %.4f\" % (global_step, epoch, loss, f1))\n",
    "            if train_bm.is_finished:\n",
    "                break\n",
    "            \n",
    "            valid_step += 1\n",
    "            if valid_step % 50 == 0:\n",
    "                valid_bm.init()\n",
    "                loss, (f1, confusion_matrix) = model.eval(sess, valid_bm, metrics)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    model.save(sess, save_path=model_path)\n",
    "                    best_flag = '*'\n",
    "                else:\n",
    "                    best_flag = ''\n",
    "                \n",
    "                time_dif = get_time_dif(start_time)\n",
    "                logging.info(\"EVALUATION: %d steps: loss %.4f  f1 %.4f  cost_time %s  %s\" % (global_step, loss, f1, str(time_dif), best_flag))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
