{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pickle\n",
    "from time import time\n",
    "import gc\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#每次可以输出多个变量\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 6\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#中文字体\n",
    "import matplotlib\n",
    "matplotlib.use('qt4agg')\n",
    "#指定默认字体\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "#解决负号'-'显示为方块的问题\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306122, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(56370, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1362492, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['target'] = -1\n",
    "df = pd.concat([train, test], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upper_num'] = df.question_text.apply(lambda x: len([i for i in x if i.isupper()]))\n",
    "df['symbol_num'] = df.question_text.apply(lambda x: len([i for i in x if i in '?!.,;[]{}:<>()@#$%^&*/+-']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can't\",\"cannot\",\"could\",\"couldn't\",\"did\",\"didn't\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn't\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\"more\",\"most\",\"mustn't\",\"my\",\"myself\",\"no\",\"nor\",\"not\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"shan't\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"wasn't\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"were\",\"weren't\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\"won't\",\"would\",\"wouldn't\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除停用词、符号\n",
    "def sent2words(sentence):\n",
    "    words = []\n",
    "    for w in wordNormal(sentence):\n",
    "        if w not in stopwords and w != '' and re.compile(r'[1-9]\\d*\\.\\d*|0\\.\\d*[1-9]|[1-9]\\d*').findall(w) == []:\n",
    "            words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def wordNormal(sentence):\n",
    "    sentence = re.sub(\"[+\\.\\!\\/_,$%^*(+\\\"\\'\\°]+|[—():~?]+{}“”&’：；;\\[\\]‘\", '', sentence)\n",
    "    sentence = re.sub(\"-\", ' ', sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged_sent = nltk.pos_tag(tokens)\n",
    "    \n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas_sent = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or nltk.corpus.wordnet.NOUN\n",
    "        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmas_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 58min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.question_text = df.question_text.apply(lambda x: sent2words(x))\n",
    "df.question_text = df.question_text.apply(lambda x: [i.lower() for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = [\"Afghanistan\",\"Anguilla\",\"Armenia\",\"Argentina\",\"Aruba\",\"Australia\",\"Austria\",\"Azerbaijan\",\"Bahamas\",\"Bahrain\",\"Bangladesh\",\"Barbados\",\"Belarus\",\"Belgium\",\"Belize\",\"Benin\",\"Bermuda\",\"Bhutan\",\"Bolivia\",\"Bouvet Islands\",\"Brazil\",\"British Indian Ocean Territory\",\"British Virgin Islands\",\"Brunei\",\"Bulgaria\",\"Burkina Faso\",\"Burundi\",\"Cambodia\",\"Cameroon\",\"Canada\",\"Cape Verde\",\"Cayman Islands\",\"Central African Republic\",\"Chad\",\"Chile\",\"China\",\"Colombia\",\"Comoros\",\"Congo\",\"Costa Rica\",\"Cote D'Ivorie\",\"Croatia\",\"Cyprus\",\"Czech Republic\",\"Denmark\",\"Djibouti\",\"Dominica\",\"Dominican Republic\",\"Egypt\",\"El Salvador\",\"Equador\",\"Equatorial Guinea\",\"Eritrea\",\"Estonia\",\"Ethiopia\",\"Falkland Islands\",\"Faroe Islands\",\"Federated States of Micronesia\",\"Fiji\",\"Finland\",\"France\",\"French Guiana\",\"French Polynesia\",\"Gabon\",\"Gambia\",\"Georgia\",\"Germany\",\"Ghana\",\"Gibraltar\",\"Greece\",\"Greenland\",\"Grenada\",\"Guadeloupe\",\"Guam\",\"Guatemala\",\"Guinea\",\"Guinea- Bissau\",\"Guyana\",\"Haiti\",\"Honduras\",\"Hong Kong\",\"Hungary\",\"Iceland\",\"India\",\"Indonesia\",\"Republic of Ireland\",\"Israel\",\"Italy\",\"Jamaica\",\"Japan\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kiribati\",\"Kuwait\",\"Kyrgyzstan\",\"Laos\",\"Latvia\",\"Lebanon\",\"Lesotho\",\"Liberia\",\"Liechtenstein\",\"Lithuania\",\"Luxembourg\",\"Macau\",\"Madagascar\",\"Malawi\",\"Malaysia\",\"Maldives\",\"Mali\",\"Malta\",\"Marshall Islands\",\"Martinique\",\"Mauritania\",\"Mayotte\",\"Metropolitan France\",\"Mexico\",\"Moldova\",\"Mongolia\",\"Morocco\",\"Mozambique\",\"Namibia\",\"Nauru\",\"Nepal\",\"Neterlands Antilles\",\"Netherlands\",\"New Caledonia\",\"New Zealand\",\"Nicaragua\",\"Niger\",\"Nigeria\",\"Northern Mariana Islands\",\"Norway\",\"Oman\",\"Pakistan\",\"Palau\",\"Panama\",\"Papua New Guinea\",\"Paraguay\",\"Peru\",\"Philippines\",\"Pitcairn\",\"Poland\",\"Portugal\",\"Puerto Rico\",\"Qatar\",\"Republic of Korea\",\"Republic of Macedonia\",\"Reunion\",\"Romania\",\"Russia\",\"Sao Tome and Principe\",\"Saudi Arabia\",\"Senegal\",\"Seychelles\",\"Singapore\",\"Slovakia\",\"Slovenia\",\"Solomon Islands\",\"Somalia\",\"South Africa\",\"Spain\",\"Sri Lanka\",\"St. Helena\",\"St. Kitts and Nevis\",\"St. Lucia\",\"St. Vincent and the Grenadines\",\"Sudan\",\"Suriname\",\"Svalbard and Jan Mayen Islands\",\"Swaziland\",\"Sweden\",\"Switzerland\",\"Syria\",\"Taiwan\",\"Tajikistan\",\"Tanzania\",\"Thailand\",\"Togo\",\"Tonga\",\"Trinidad and Tobago\",\"Turkey\",\"Turkmenistan\",\"Turks and Caicos Islands\",\"Tuvalu\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States USA\",\"Uruguay\",\"Uzbekistan\",\"Vanuatu\",\"Vatican City\",\"Venezuela\",\"Vietnam\",\"Western Sahara\",\"Yemen\",\"Yugoslavia\",\"Zaire\",\"Zambia\",\"Zimbabwe\",\"North Korea\"]\n",
    "country_short = [\"AF\",\"AI\",\"AM\",\"AR\",\"AW\",\"AU\",\"AT\",\"AZ\",\"BS\",\"BH\",\"BD\",\"BB\",\"BY\",\"BE\",\"BZ\",\"BJ\",\"BM\",\"BT\",\"BO\",\"BV\",\"BR\",\"IO\",\"VI\",\"BN\",\"BG\",\"BF\",\"BI\",\"KH\",\"CM\",\"CA\",\"CV\",\"KY\",\"CF\",\"TD\",\"CL\",\"CN\",\"CO\",\"KM\",\"CG\",\"CR\",\"CI\",\"HR\",\"CY\",\"CZ\",\"DK\",\"DJ\",\"DM\",\"DO\",\"EG\",\"SV\",\"EC\",\"GQ\",\"ER\",\"EE\",\"ET\",\"FK\",\"FO\",\"FM\",\"FJ\",\"FI\",\"FR\",\"GF\",\"PF\",\"GA\",\"GM\",\"GE\",\"DE\",\"GH\",\"GI\",\"GR\",\"GL\",\"GD\",\"GP\",\"GU\",\"GT\",\"GN\",\"GW\",\"GY\",\"HT\",\"HN\",\"HK\",\"HU\",\"IS\",\"IN\",\"ID\",\"IE\",\"IL\",\"IT\",\"JM\",\"JP\",\"JO\",\"KZ\",\"KE\",\"KI\",\"KW\",\"KG\",\"LA\",\"LV\",\"LB\",\"LS\",\"LR\",\"LI\",\"LT\",\"LU\",\"MO\",\"MG\",\"MW\",\"MY\",\"MV\",\"ML\",\"MT\",\"MH\",\"MQ\",\"MR\",\"YT\",\"FX\",\"MX\",\"MD\",\"MN\",\"MA\",\"MZ\",\"NA\",\"NR\",\"NP\",\"AN\",\"NL\",\"NC\",\"NZ\",\"NI\",\"NE\",\"NG\",\"MP\",\"NO\",\"OM\",\"PK\",\"PW\",\"PA\",\"PG\",\"PY\",\"PE\",\"PH\",\"PN\",\"PL\",\"PT\",\"PR\",\"QA\",\"KR\",\"MK\",\"RE\",\"RO\",\"RU\",\"ST\",\"SA\",\"SN\",\"SC\",\"SG\",\"SK\",\"SI\",\"SB\",\"SO\",\"ZA\",\"ES\",\"LK\",\"SH\",\"KN\",\"LC\",\"VC\",\"SD\",\"SR\",\"SJ\",\"SZ\",\"SE\",\"CH\",\"SY\",\"TW\",\"TJ\",\"TZ\",\"TH\",\"TG\",\"TO\",\"TT\",\"TR\",\"TM\",\"TC\",\"TV\",\"UG\",\"UA\",\"AE\",\"GB\",\"US\",\"UY\",\"UZ\",\"VU\",\"VA\",\"VE\",\"VN\",\"EH\",\"YE\",\"YU\",\"ZR\",\"ZM\",\"ZW\"]\n",
    "religion = [\"Ahmadiyya\",\"Aladura\",\"Amish\",\"Anglicanism\",\"Asatru\",\"Assemblies of God\",\"Atheism\",\"Baha'i Faith\",\"Baptists\",\"Bon\",\"Buddhism\",\"Candomble\",\"Cao Dai\",\"Cathari\",\"Catholicism\",\"Charismatic movement\",\"Chinese Religion\",\"Christadelphians\",\"Christian Science\",\"Christianity\",\"Church of God\",\"Church of God in Christ\",\"Church of Satan\",\"Confucianism\",\"Conservative Judaism\",\"Deism\",\"Donatism\",\"Dragon Rouge\",\"Druze\",\"Eastern Orthodox Church\",\"Eckankar\",\"ELCA\",\"Epicureanism\",\"Evangelicalism\",\"Falun Gong\",\"Foursquare Church\",\"Gnosticism\",\"Greek Religion\",\"Hare Krishna\",\"Hasidism\",\"Hellenic Reconstructionism\",\"Hinduism\",\"Illuminati\",\"Intelligent Design\",\"Islam\",\"Jainism\",\"Jehovah's Witnesses\",\"Judaism\",\"Kabbalah\",\"Kemetic Reconstructionism\",\"Lutheranism\",\"Mahayana Buddhism\",\"Mayan Religion\",\"Methodism\",\"Mithraism\",\"Mormonism\",\"Neopaganism\",\"New Age\",\"New Thought\",\"Nichiren\",\"Norse Religion\",\"Olmec Religion\",\"Oneness Pentecostalism\",\"Orthodox Judaism\",\"Pentecostalism\",\"Presbyterianism\",\"Priory of Sion\",\"Protestantism\",\"Pure Land Buddhism\",\"Quakers\",\"Rastafarianism\",\"Reform Judaism\",\"Rinzai Zen Buddhism\",\"Roman Religion\",\"Satanism\",\"Scientology\",\"Seventh-Day Adventism\",\"Shaivism\",\"Shi'a Islam\",\"Shinto\",\"Sikhism\",\"Soto Zen Buddhism\",\"Spiritualism\",\"Stoicism\",\"Sufism\",\"Sunni Islam\",\"Taoism\",\"Tendai Buddhism\",\"Theravada Buddhism\",\"Tibetan Buddhism\",\"Typhonian Order\",\"Umbanda\",\"Unification Church\",\"Unitarian Universalism\",\"Vaishnavism\",\"Vajrayana Buddhism\",\"Vedanta\",\"Vineyard Churches\",\"Voodoo\",\"Westboro Baptist Church\",\"Wicca\",\"Worldwide Church of God\",\"Yezidi\",\"Zen\",\"Zionism\",\"Zoroastrianism\"]\n",
    "\n",
    "country = [i.lower() for i in country]\n",
    "country_short = [i.lower() for i in country_short]\n",
    "religion = [i.lower() for i in religion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['has_country_num'] = df.question_text.apply(lambda x: len(set(country) & set(x)))\n",
    "df['has_country_short_num'] = df.question_text.apply(lambda x: len(set(country_short) & set(x)))\n",
    "df['has_religion_num'] = df.question_text.apply(lambda x: len(set(religion) & set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异常问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1225312\n",
       " 1      80810\n",
       "-1      56370\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>upper_num</th>\n",
       "      <th>symbol_num</th>\n",
       "      <th>has_country_num</th>\n",
       "      <th>has_country_short_num</th>\n",
       "      <th>has_religion_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0000e91571b60c2fb487</td>\n",
       "      <td>[has, united, states, become, large, dictators...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>00013ceca3f624b09f42</td>\n",
       "      <td>[which, baby, sweet, parent, dark, skin, baby,...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0004a7fcb2bf73076489</td>\n",
       "      <td>[if, black, support, school, choice, mandatory...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>00052793eaa287aff1e1</td>\n",
       "      <td>[i, gay, boy, i, love, cousin, boy, he, sexy, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>000537213b01fd77b58a</td>\n",
       "      <td>[which, race, small, penis]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      qid                                      question_text  \\\n",
       "22   0000e91571b60c2fb487  [has, united, states, become, large, dictators...   \n",
       "30   00013ceca3f624b09f42  [which, baby, sweet, parent, dark, skin, baby,...   \n",
       "110  0004a7fcb2bf73076489  [if, black, support, school, choice, mandatory...   \n",
       "114  00052793eaa287aff1e1  [i, gay, boy, i, love, cousin, boy, he, sexy, ...   \n",
       "115  000537213b01fd77b58a                        [which, race, small, penis]   \n",
       "\n",
       "     target  upper_num  symbol_num  has_country_num  has_country_short_num  \\\n",
       "22        1          3           1                0                      0   \n",
       "30        1          2           2                0                      0   \n",
       "110       1          2           1                0                      0   \n",
       "114       1          8          10                0                      0   \n",
       "115       1          1           1                0                      0   \n",
       "\n",
       "     has_religion_num  \n",
       "22                  0  \n",
       "30                  0  \n",
       "110                 0  \n",
       "114                 0  \n",
       "115                 0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()\n",
    "df[df.target == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b605b20ed91841fd8eae5cef115b6822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "except_word_dict = defaultdict(lambda : 0)\n",
    "\n",
    "for i in tqdm_notebook(df[df.target == 1].question_text):\n",
    "    for word in i:\n",
    "        except_word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04677686524247088a01cf4710cad644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_dict = defaultdict(lambda : 0)\n",
    "\n",
    "for i in tqdm_notebook(df.question_text):\n",
    "    for word in i:\n",
    "        word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb79377fcd549459a49e12a091a4712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "except_word_dict_rate = defaultdict(lambda : 0)\n",
    "\n",
    "for i in tqdm_notebook(except_word_dict.keys()):\n",
    "    except_word_dict_rate[i] = except_word_dict[i] / word_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "except_word = ['woman','trump','men','white','muslims','black','girl','india','indian','americans','us','sex','indians','liberal','chinese','muslim','american','president','child','gay','america','donald','old','jews','man','racist','god','government','china','obama','christians','democrats','hindus','state','religion','human','stupid','islam','usa','modi','pakistan','israel','conservative','anti','male','atheist','terrorist','money','rape','female','hillary','british','asian','russia','united','jewish','african','sexual','feminist','western','hindu','immigrant','fuck','clinton','europe','christian','kid','penis','uk','arent','japanese','force','republicans','russian','trumps','europeans','bjp','religious','political','hitler','european','west','asians','democrat','castrate','pakistani','republican','jesus','minority','enough','democratic','slave','dumb','palestinians','korea','islamic','abuse','suck','liberals','homosexual','tamil','pakistanis','russians','shit','criminal','victim','holocaust','canada','iran','transgender','sexually','terrorism','leftist','africans','africa','foreign','christianity','israeli','korean','homosexuality','jew','masturbate','japan','kashmir','quorans','gandhi','syria','nazis','australia','germans','french','girls','drug','britain','fbi','german','brahmins','eu','muhammad','vagina','canadians','lgbt','barack','turkish','koreans','pussy','england','brainwash','mexico','quran','asshole','hinduism','vietnamese','ive','mexicans','sexist','palestinian','christ','france','sweden','narcissist','obamas','karnataka','harry','bangladesh','naked','punishment','california','iranians','bengali','rahul','mexican','union','kashmiri','pedophile','jerusalem','harass','delhi','nationalist','arabia','nepal','iraq','australians','gods','australian','prostitute','italians','canadian','bengal','dalits','catholics','tamilians','xi','melania','jinping','cnn']\n",
    "except_word_rate1 = [i[0] for i in sorted(except_word_dict_rate.items(), key=lambda x:x[1], reverse=True) if (i[1] == 1) and (word_dict[i[0]] > 1)]\n",
    "except_word_rate2 = [i[0] for i in sorted(except_word_dict_rate.items(), key=lambda x:x[1], reverse=True) if (i[1] == 1) and (word_dict[i[0]] > 2)]\n",
    "len(except_word_rate1)\n",
    "len(except_word_rate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df['has_except_word_num'] = df.question_text.apply(lambda x: len(set(except_word) & set(x)))\n",
    "df['has_except_word_rate1_num'] = df.question_text.apply(lambda x: len(set(except_word_rate1) & set(x)))\n",
    "df['has_except_word_rate2_num'] = df.question_text.apply(lambda x: len(set(except_word_rate2) & set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6912, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(73898, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.has_except_word_rate_num != 0) & (df.target == 1)].shape\n",
    "df[(df.has_except_word_rate_num == 0) & (df.target == 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>upper_num</th>\n",
       "      <th>symbol_num</th>\n",
       "      <th>has_country_num</th>\n",
       "      <th>has_country_short_num</th>\n",
       "      <th>has_religion_num</th>\n",
       "      <th>has_except_word_num</th>\n",
       "      <th>has_except_word_rate1_num</th>\n",
       "      <th>has_except_word_rate2_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>[how, quebec, nationalist, see, province, nation]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>[do, adopt, dog, encourage, people, adopt, shop]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>[why, velocity, affect, time, does, velocity, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>[how, otto, von, guericke, use, magdeburg, hem...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>[can, i, convert, montra, helicon, d, mountain...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  [how, quebec, nationalist, see, province, nation]   \n",
       "1  000032939017120e6e44   [do, adopt, dog, encourage, people, adopt, shop]   \n",
       "2  0000412ca6e4628ce2cf  [why, velocity, affect, time, does, velocity, ...   \n",
       "3  000042bf85aa498cd78e  [how, otto, von, guericke, use, magdeburg, hem...   \n",
       "4  0000455dfa3e01eae3af  [can, i, convert, montra, helicon, d, mountain...   \n",
       "\n",
       "   target  upper_num  symbol_num  has_country_num  has_country_short_num  \\\n",
       "0       0          2           1                0                      0   \n",
       "1       0          1           2                0                      1   \n",
       "2       0          2           2                0                      0   \n",
       "3       0          4           1                0                      0   \n",
       "4       0          3           1                0                      0   \n",
       "\n",
       "   has_religion_num  has_except_word_num  has_except_word_rate1_num  \\\n",
       "0                 0                    1                          0   \n",
       "1                 0                    0                          0   \n",
       "2                 0                    0                          0   \n",
       "3                 0                    0                          0   \n",
       "4                 0                    0                          0   \n",
       "\n",
       "   has_except_word_rate2_num  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
