{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#每次可以输出多个变量\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tensorboard查看网络结构\n",
    "tensorboard --logdir=/home/zero/tf/logs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0 test acc: 0.7411\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 命名空间\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, len(mnist.train.images[0])], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, len(mnist.train.labels[0])], name='y-input')\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    # 构建神经网络\n",
    "    with tf.name_scope('weight'):\n",
    "        W = tf.Variable(tf.zeros([len(mnist.train.images[0]), len(mnist.train.labels[0])]), name='W')\n",
    "    with tf.name_scope('bias'):\n",
    "        b = tf.Variable(tf.zeros([10]), name='b')\n",
    "    with tf.name_scope('output'):\n",
    "        output = tf.matmul(x, W) + b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(output)\n",
    "\n",
    "        \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "with tf.name_scope('trian'):\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 准确率\n",
    "with tf.name_scope('accuary'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(prediction, axis=1))\n",
    "    with tf.name_scope('acc'):\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(init)\n",
    "for epoch in range(1):\n",
    "    # 将图存入文件\n",
    "    writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "    for batch in range(n_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train, feed_dict={x:batch_xs, y:batch_ys})\n",
    "    print('Iter ' + str(epoch) + ' test acc: ' + str(sess.run(acc, feed_dict={x:mnist.test.images, y:mnist.test.labels})))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tensorboard查看网络运行过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss/loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuary/acc/acc:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 test acc: 0.7365\n",
      "Iter 1 test acc: 0.8314\n",
      "Iter 2 test acc: 0.8592\n",
      "Iter 3 test acc: 0.8701\n",
      "Iter 4 test acc: 0.8775\n",
      "Iter 5 test acc: 0.8822\n",
      "Iter 6 test acc: 0.8842\n",
      "Iter 7 test acc: 0.8874\n",
      "Iter 8 test acc: 0.8911\n",
      "Iter 9 test acc: 0.8942\n",
      "Iter 10 test acc: 0.8953\n",
      "Iter 11 test acc: 0.8978\n",
      "Iter 12 test acc: 0.8985\n",
      "Iter 13 test acc: 0.8997\n",
      "Iter 14 test acc: 0.9004\n",
      "Iter 15 test acc: 0.9015\n",
      "Iter 16 test acc: 0.9024\n",
      "Iter 17 test acc: 0.9036\n",
      "Iter 18 test acc: 0.9039\n",
      "Iter 19 test acc: 0.9049\n",
      "Iter 20 test acc: 0.9057\n",
      "Iter 21 test acc: 0.9065\n",
      "Iter 22 test acc: 0.907\n",
      "Iter 23 test acc: 0.9072\n",
      "Iter 24 test acc: 0.907\n",
      "Iter 25 test acc: 0.9085\n",
      "Iter 26 test acc: 0.9088\n",
      "Iter 27 test acc: 0.9098\n",
      "Iter 28 test acc: 0.9099\n",
      "Iter 29 test acc: 0.9096\n",
      "Iter 30 test acc: 0.9103\n",
      "Iter 31 test acc: 0.9107\n",
      "Iter 32 test acc: 0.9113\n",
      "Iter 33 test acc: 0.9119\n",
      "Iter 34 test acc: 0.9118\n",
      "Iter 35 test acc: 0.9125\n",
      "Iter 36 test acc: 0.9128\n",
      "Iter 37 test acc: 0.9129\n",
      "Iter 38 test acc: 0.9126\n",
      "Iter 39 test acc: 0.9129\n",
      "Iter 40 test acc: 0.9137\n",
      "Iter 41 test acc: 0.9138\n",
      "Iter 42 test acc: 0.914\n",
      "Iter 43 test acc: 0.9146\n",
      "Iter 44 test acc: 0.9147\n",
      "Iter 45 test acc: 0.9149\n",
      "Iter 46 test acc: 0.9155\n",
      "Iter 47 test acc: 0.9163\n",
      "Iter 48 test acc: 0.9156\n",
      "Iter 49 test acc: 0.9161\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 命名空间\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, len(mnist.train.images[0])], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, len(mnist.train.labels[0])], name='y-input')\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    # 构建神经网络\n",
    "    with tf.name_scope('weight'):\n",
    "        W = tf.Variable(tf.zeros([len(mnist.train.images[0]), len(mnist.train.labels[0])]), name='W')\n",
    "        variable_summaries(W)\n",
    "    with tf.name_scope('bias'):\n",
    "        b = tf.Variable(tf.zeros([len(mnist.train.labels[0])]), name='b')\n",
    "        variable_summaries(b)\n",
    "    with tf.name_scope('output'):\n",
    "        output = tf.matmul(x, W) + b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(output)\n",
    "\n",
    "        \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "with tf.name_scope('trian'):\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 准确率\n",
    "with tf.name_scope('accuary'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(prediction, axis=1))\n",
    "    with tf.name_scope('acc'):\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "        tf.summary.scalar('acc', acc)\n",
    "        \n",
    "# 合并所有的summary，并使用fetch放入训练过程\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(init)\n",
    "for epoch in range(50):\n",
    "    # 将图存入文件\n",
    "    writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "    for batch in range(n_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        summary, _ = sess.run([merged, train], feed_dict={x:batch_xs, y:batch_ys})\n",
    "    \n",
    "    # 将summary写入文件\n",
    "    writer.add_summary(summary, epoch)\n",
    "    print('Iter ' + str(epoch) + ' test acc: ' + str(sess.run(acc, feed_dict={x:mnist.test.images, y:mnist.test.labels})))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tesnsorboard可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 参数概要\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f100684694ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 载入数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MNIST_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 运行次数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[0;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed, source_url)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m   \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images, labels, fake_data, one_hot, dtype, reshape, seed)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Convert from [0, 255] -> [0.0, 1.0].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "# 运行次数\n",
    "max_steps = 1000\n",
    "# 图片数量\n",
    "image_num = 3000\n",
    "    \n",
    "# 载入图片\n",
    "embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]), trainable=False, name = 'embedding')\n",
    "\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 命名空间\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, len(mnist.train.images[0])], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, len(mnist.train.labels[0])], name='y-input')\n",
    "    \n",
    "# 显示图片\n",
    "with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1]) # 黑白图片为1，彩色图片为3\n",
    "    tf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "with tf.name_scope('layer'):\n",
    "    # 构建神经网络\n",
    "    with tf.name_scope('weight'):\n",
    "        W = tf.Variable(tf.zeros([len(mnist.train.images[0]), len(mnist.train.labels[0])]), name='W')\n",
    "        variable_summaries(W)\n",
    "    with tf.name_scope('bias'):\n",
    "        b = tf.Variable(tf.zeros([len(mnist.train.labels[0])]), name='b')\n",
    "        variable_summaries(b)\n",
    "    with tf.name_scope('output'):\n",
    "        output = tf.matmul(x, W) + b\n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(output)\n",
    "\n",
    "        \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "with tf.name_scope('trian'):\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 准确率\n",
    "with tf.name_scope('accuary'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(prediction, axis=1))\n",
    "    with tf.name_scope('acc'):\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "        tf.summary.scalar('acc', acc)\n",
    "        \n",
    "# 合并所有的summary，并使用fetch放入训练过程\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# 生成metadata文件\n",
    "if tf.gfile.Exists('./projector/metadata.csv'):\n",
    "    tf.gfile.DeleteRecursively('./projector/metadata.csv')\n",
    "with open('./projector/metadata.csv', 'w') as f:\n",
    "    labels = sess.run(tf.argmax(mnist.test.labels[:], 1))\n",
    "    for i in range(image_num):\n",
    "        _ = f.write(str(labels[i]) + '\\n')\n",
    "\n",
    "# 可视化配置\n",
    "projector_writer = tf.summary.FileWriter('./projector/', sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "config = projector.ProjectorConfig()\n",
    "embed = config.embeddings.add()\n",
    "embed.tensor_name = embedding.name\n",
    "embed.metadata_path = './projector/metadata.csv'\n",
    "embed.sprite.image_path = './projector/mnist_10k_sprite.png'\n",
    "embed.sprite.single_image_dim.extend([28, 28])\n",
    "projector.visualize_embeddings(projector_writer, config)\n",
    "\n",
    "sess.run(init)\n",
    "for epoch in range(max_steps):\n",
    "    # 每次训练100张图片\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "    run_metadata = tf.RunMetadata()\n",
    "    summary, _ = sess.run([merged, train], feed_dict={x:batch_xs, y:batch_ys}, options=run_options, run_metadata=run_metadata)\n",
    "    projector_writer.add_run_metadata(run_metadata, 'step%03d' % epoch)\n",
    "    projector_writer.add_summary(summary, epoch)\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('Iter ' + str(epoch) + ' test acc: ' + str(sess.run(acc, feed_dict={x:mnist.test.images, y:mnist.test.labels})))\n",
    "\n",
    "saver.save(sess, './projector/a_model.ckpt', global_step=max_steps)\n",
    "projector_writer.close()\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
