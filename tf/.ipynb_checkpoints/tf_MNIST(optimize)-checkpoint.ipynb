{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#每次可以输出多个变量\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_x = len(mnist.train.images[0])\n",
    "image_y = len(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, image_x])\n",
    "y = tf.placeholder(tf.float32, [None, image_y])\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 动态改变学习率\n",
    "lr = tf.Variable(0.001, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 原始\n",
    "W = tf.Variable(tf.zeros([image_x, image_y]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 优化\n",
    "layer_cnt1 = 2000\n",
    "layer_cnt2 = 2000\n",
    "layer_cnt3 = 2000\n",
    "layer_cnt4 = 2000\n",
    "layer_cnt5 = 1000\n",
    "bias = 0.1\n",
    "\n",
    "# 初始化（权值使用截断的正态分布，偏置使用0.1）\n",
    "weight_l1 = tf.Variable(tf.truncated_normal([image_x, layer_cnt1], stddev=0.1))\n",
    "bias_l1 = tf.Variable(tf.zeros([layer_cnt1]) + bias)\n",
    "input_l1 = tf.matmul(x, weight_l1) + bias_l1\n",
    "output_l1 = tf.nn.tanh(input_l1)\n",
    "l1_dropout = tf.nn.dropout(output_l1, keep_prob)\n",
    "\n",
    "weight_l2 = tf.Variable(tf.truncated_normal([layer_cnt1, layer_cnt2], stddev=0.1))\n",
    "bias_l2 = tf.Variable(tf.zeros([layer_cnt2]) + bias)\n",
    "input_l2 = tf.matmul(l1_dropout, weight_l2) + bias_l2\n",
    "output_l2 = tf.nn.tanh(input_l2)\n",
    "l2_dropout = tf.nn.dropout(output_l2, keep_prob)\n",
    "\n",
    "weight_l3 = tf.Variable(tf.truncated_normal([layer_cnt2, layer_cnt3], stddev=0.1))\n",
    "bias_l3 = tf.Variable(tf.zeros([layer_cnt3]) + bias)\n",
    "input_l3 = tf.matmul(l2_dropout, weight_l3) + bias_l3\n",
    "output_l3 = tf.nn.tanh(input_l3)\n",
    "l3_dropout = tf.nn.dropout(output_l3, keep_prob)\n",
    "\n",
    "weight_l4 = tf.Variable(tf.truncated_normal([layer_cnt3, layer_cnt4], stddev=0.1))\n",
    "bias_l4 = tf.Variable(tf.zeros([layer_cnt4]) + bias)\n",
    "input_l4 = tf.matmul(l3_dropout, weight_l4) + bias_l4\n",
    "output_l4 = tf.nn.tanh(input_l4)\n",
    "l4_dropout = tf.nn.dropout(output_l4, keep_prob)\n",
    "\n",
    "weight_l5 = tf.Variable(tf.truncated_normal([layer_cnt4, layer_cnt5], stddev=0.1))\n",
    "bias_l5 = tf.Variable(tf.zeros([layer_cnt5]) + bias)\n",
    "input_l5 = tf.matmul(l2_dropout, weight_l5) + bias_l5\n",
    "output_l5 = tf.nn.tanh(input_l5)\n",
    "l5_dropout = tf.nn.dropout(output_l5, keep_prob)\n",
    "\n",
    "weight_l6 = tf.Variable(tf.truncated_normal([layer_cnt5, image_y], stddev=0.1))\n",
    "bias_l6 = tf.Variable(tf.zeros([image_y]) + bias)\n",
    "input_l6 = tf.matmul(l5_dropout, weight_l6) + bias_l6\n",
    "output_l6 = tf.nn.softmax(input_l6)\n",
    "\n",
    "prediction = output_l6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss = tf.reduce_mean(tf.square(y - prediction)) # 二次代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) # 交叉熵代价函数\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1) # SGD\n",
    "# optimizer = tf.train.AdadeltaOptimizer() # Adadelta\n",
    "optimizer = tf.train.AdamOptimizer(lr) # Adam\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准确率\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(prediction, axis=1))\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0->train acc:0.906727,test acc:0.9105\n",
      "Iter 1->train acc:0.913109,test acc:0.9152\n",
      "Iter 2->train acc:0.925909,test acc:0.9285\n",
      "Iter 3->train acc:0.930909,test acc:0.9335\n",
      "Iter 4->train acc:0.935055,test acc:0.9383\n",
      "Iter 5->train acc:0.942018,test acc:0.9429\n",
      "Iter 6->train acc:0.942964,test acc:0.9414\n",
      "Iter 7->train acc:0.947618,test acc:0.947\n",
      "Iter 8->train acc:0.9494,test acc:0.9472\n",
      "Iter 9->train acc:0.950727,test acc:0.9489\n",
      "Iter 10->train acc:0.953418,test acc:0.9503\n",
      "Iter 11->train acc:0.954291,test acc:0.9516\n",
      "Iter 12->train acc:0.956709,test acc:0.9525\n",
      "Iter 13->train acc:0.957364,test acc:0.9536\n",
      "Iter 14->train acc:0.9582,test acc:0.9552\n",
      "Iter 15->train acc:0.961309,test acc:0.9583\n",
      "Iter 16->train acc:0.961455,test acc:0.957\n",
      "Iter 17->train acc:0.963036,test acc:0.9591\n",
      "Iter 18->train acc:0.9636,test acc:0.9591\n",
      "Iter 19->train acc:0.966182,test acc:0.9619\n",
      "Iter 20->train acc:0.966782,test acc:0.963\n",
      "Iter 21->train acc:0.967491,test acc:0.9631\n",
      "Iter 22->train acc:0.967964,test acc:0.964\n",
      "Iter 23->train acc:0.9686,test acc:0.9635\n",
      "Iter 24->train acc:0.969473,test acc:0.9635\n",
      "Iter 25->train acc:0.971,test acc:0.9646\n",
      "Iter 26->train acc:0.970836,test acc:0.9655\n",
      "Iter 27->train acc:0.971945,test acc:0.9668\n",
      "Iter 28->train acc:0.971836,test acc:0.965\n",
      "Iter 29->train acc:0.972618,test acc:0.9656\n",
      "Iter 30->train acc:0.973055,test acc:0.9681\n",
      "Iter 31->train acc:0.973564,test acc:0.967\n",
      "Iter 32->train acc:0.973836,test acc:0.967\n",
      "Iter 33->train acc:0.9748,test acc:0.9666\n",
      "Iter 34->train acc:0.975091,test acc:0.9675\n",
      "Iter 35->train acc:0.975691,test acc:0.97\n",
      "Iter 36->train acc:0.975855,test acc:0.9691\n",
      "Iter 37->train acc:0.976473,test acc:0.9681\n",
      "Iter 38->train acc:0.976291,test acc:0.969\n",
      "Iter 39->train acc:0.976909,test acc:0.9697\n",
      "Iter 40->train acc:0.976782,test acc:0.9699\n",
      "Iter 41->train acc:0.977236,test acc:0.9706\n",
      "Iter 42->train acc:0.977855,test acc:0.9707\n",
      "Iter 43->train acc:0.978036,test acc:0.9708\n",
      "Iter 44->train acc:0.978455,test acc:0.9701\n",
      "Iter 45->train acc:0.978382,test acc:0.9717\n",
      "Iter 46->train acc:0.978382,test acc:0.9705\n",
      "Iter 47->train acc:0.978745,test acc:0.9706\n",
      "Iter 48->train acc:0.978691,test acc:0.9703\n",
      "Iter 49->train acc:0.978782,test acc:0.9705\n",
      "Iter 50->train acc:0.979127,test acc:0.9699\n",
      "Iter 51->train acc:0.979673,test acc:0.9708\n",
      "Iter 52->train acc:0.980055,test acc:0.9705\n",
      "Iter 53->train acc:0.980255,test acc:0.9705\n",
      "Iter 54->train acc:0.980345,test acc:0.9709\n",
      "Iter 55->train acc:0.980436,test acc:0.9716\n",
      "Iter 56->train acc:0.980927,test acc:0.9717\n",
      "Iter 57->train acc:0.980582,test acc:0.9709\n",
      "Iter 58->train acc:0.980709,test acc:0.9717\n",
      "Iter 59->train acc:0.981036,test acc:0.9722\n",
      "Iter 60->train acc:0.981327,test acc:0.9717\n",
      "Iter 61->train acc:0.981164,test acc:0.9724\n",
      "Iter 62->train acc:0.981473,test acc:0.9718\n",
      "Iter 63->train acc:0.981764,test acc:0.9715\n",
      "Iter 64->train acc:0.981582,test acc:0.9719\n",
      "Iter 65->train acc:0.981727,test acc:0.9724\n",
      "Iter 66->train acc:0.981964,test acc:0.972\n",
      "Iter 67->train acc:0.982036,test acc:0.9721\n",
      "Iter 68->train acc:0.982055,test acc:0.9723\n",
      "Iter 69->train acc:0.982055,test acc:0.9724\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(init)\n",
    "for epoch in range(70):\n",
    "    _ = sess.run((tf.assign(lr, 0.001 * (0.95 ** epoch))))\n",
    "    for batch in range(n_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # 训练时dropout设为较小的浮点数，最终运行模型时依然设为1\n",
    "        sess.run(train, feed_dict={x:batch_xs, y:batch_ys, keep_prob:0.5})\n",
    "#     print('Iter ' + str(epoch) + '->train acc:' + str(sess.run(acc, feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:0.3})) + ',test acc:' + str(sess.run(acc, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:0.3})))\n",
    "    print('Iter ' + str(epoch) + '->train acc:' + str(sess.run(acc, feed_dict={x:mnist.train.images, y:mnist.train.labels, keep_prob:1.0})) + ',test acc:' + str(sess.run(acc, feed_dict={x:mnist.test.images, y:mnist.test.labels, keep_prob:1.0})))\n",
    "#     print(' ')\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调优结论\n",
    "- 交叉熵代价函数比二次代价函数使模型更快收敛，且准确率更高\n",
    "- 增加dropout会使迭代速度变慢，但是和train的准确率差距变小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 防止过拟合\n",
    "- 增加数据集\n",
    "- 正则化\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "![SGD](./img/sgd.png)\n",
    "- SGD:朝着梯度的反方向（优化速度最快）优化w和b。\n",
    "\n",
    "\n",
    "![Momentum](./img/momentum.png)\n",
    "- Momentum:在SGD基础上增加一个动力项（通常设成0.9），当前权值改变受到上一次权值改变影响，加快收敛速度。\n",
    "\n",
    "\n",
    "![NAG](./img/nag.png)\n",
    "- NAG(Nesterov accelerated gradient):在计算参数时也减去动力项，使收敛更加精确。\n",
    "\n",
    "\n",
    "![Adagrad](./img/adagrad.png)\n",
    "- Adagrad:比较常见的数据用较小的学习率调整参数，罕见的数据使用较大的学习率调整参数，适用于参数不平衡的数据集，但是随着迭代次数增多会使学习率逐渐趋向于0。\n",
    "\n",
    "\n",
    "![RMS](./img/RMSprop.png)\n",
    "- RMSprop(Root Mean Square均方根):前t次梯度平方的均值加上当前梯度的平方根作为学习率的分母，不会出现学习率减小的问题。\n",
    "\n",
    "\n",
    "![Adadelta](./img/adadelta.png)\n",
    "- Adadelta:不用设置学习率也能得到一个很好的效果。\n",
    "\n",
    "\n",
    "![Adam](./img/adam.png)\n",
    "- Adam:存储之前衰减的平方梯度，自动调整学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Compare](./img/compare.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
