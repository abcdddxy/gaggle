{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import pickle\n",
    "from time import time\n",
    "import gc\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#每次可以输出多个变量\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 6\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#中文字体\n",
    "import matplotlib\n",
    "matplotlib.use('qt4agg')\n",
    "#指定默认字体\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "#解决负号'-'显示为方块的问题\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import math\n",
    "import logging\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "from scipy.sparse import hstack, vstack\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#每次可以输出多个变量\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MLP(inp, hidden_dims):\n",
    "    x = tf.layers.Dense(hidden_dims[0], kernel_initializer=tf.keras.initializers.he_normal(), dtype=tf.float32, activation=tf.nn.leaky_relu)(inp)\n",
    "    x = tf.layers.BatchNormalization(dtype=tf.float32)(x)\n",
    "    x = tf.nn.leaky_relu(x)\n",
    "    for i, dim in enumerate(hidden_dims):\n",
    "        if i > 0:\n",
    "            x = tf.layers.Dense(dim, kernel_initializer=tf.keras.initializers.he_normal(), dtype=tf.float32, activation=tf.nn.leaky_relu)(x)\n",
    "            x = tf.layers.BatchNormalization(dtype=tf.float32)(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DCFN:\n",
    "    def __init__(self, learning_rate, embedding_size, dnn_layers, att_layer, cross_layer_num, conti_fea_cnt,\n",
    "                 cate_embedding_uni_cnt_list, cate_embedding_w_list=None, fm_embedding_w=None, no_nan_w=None,\n",
    "                 nan_w=None, fm_drop_outs=[1, 1]):\n",
    "        self.lr = learning_rate\n",
    "        self.conti_fea_cnt = conti_fea_cnt\n",
    "        self.embedding_size = embedding_size\n",
    "        self.fm_drop_outs = fm_drop_outs\n",
    "        self.dnn_layers = dnn_layers\n",
    "        self.att_layer = att_layer\n",
    "        self.cross_layer_num = cross_layer_num\n",
    "        # cate_embedding_uni_cnt_list离散特征计数\n",
    "        self.cate_embedding_uni_cnt_list = cate_embedding_uni_cnt_list\n",
    "        self.cate_embedding_w_list = cate_embedding_w_list\n",
    "\n",
    "        self.fm_embedding_w = fm_embedding_w\n",
    "        self.no_nan_w = no_nan_w\n",
    "        self.nan_w = nan_w\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            config = tf.ConfigProto()\n",
    "            config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(config=config)\n",
    "\n",
    "            self.input_vecs = []\n",
    "\n",
    "            self.conti_vec = tf.placeholder(tf.float32, shape=[None, self.conti_fea_cnt], name='conti_vec')\n",
    "            self.cate_indexs = tf.placeholder(tf.int16, shape=[None, sum(self.cate_embedding_uni_cnt_list)],\n",
    "                                              name='cate_indexs')\n",
    "            self.label = tf.placeholder(dtype=tf.int8, shape=[None, 1], name='label')\n",
    "\n",
    "            self.cate_embeddings = []\n",
    "            self.fm_fea_size = 0\n",
    "\n",
    "            # 第一层embedding：降维\n",
    "            cate_offset = 0\n",
    "            for cate_idx, uni_cnt in enumerate(self.cate_embedding_uni_cnt_list):\n",
    "                w = self.cate_embedding_w_list[cate_idx] if self.cate_embedding_w_list else tf.keras.initializers.he_normal()\n",
    "                embedding_k = uni_cnt if int(2 * np.power(uni_cnt, 1 / 4)) > uni_cnt else int(\n",
    "                    2 * np.power(uni_cnt, 1 / 4))\n",
    "                self.fm_fea_size += embedding_k\n",
    "                # embedding矩阵\n",
    "                self.cate_embeddings.append(\n",
    "                    tf.get_variable('cate_%d_embedding' % cate_idx, shape=[uni_cnt, embedding_k], dtype=tf.float32,\n",
    "                                    initializer=w))\n",
    "\n",
    "                crt_vec_index = self.cate_indexs[:, cate_offset:cate_offset + uni_cnt]  # None * uni_cnt\n",
    "                cate_offset += uni_cnt\n",
    "                crt_vec_index = tf.Print(crt_vec_index, [crt_vec_index], message='Debug:', summarize=50)\n",
    "\n",
    "                crt_vec = tf.nn.embedding_lookup(self.cate_embeddings[cate_idx],\n",
    "                                                 [i for i in range(uni_cnt)])  # uni_cnt * K\n",
    "                # 等于1的加起来，求平均（embedding相当于多行相加，multi-hot要除1的个数保证一致）\n",
    "                crt_vec = tf.matmul(tf.cast(crt_vec_index, tf.float32), crt_vec)  # None * K\n",
    "                one_cnt = tf.cast(tf.reduce_sum(crt_vec_index, axis=1, keep_dims=True), dtype=tf.float32)  # None * 1\n",
    "                crt_vec = tf.div(crt_vec, one_cnt)  # None * K\n",
    "                self.input_vecs.append(crt_vec)\n",
    "\n",
    "            mv_conti_vec = self.conti_vec\n",
    "#             with tf.variable_scope('Missing-Value-Layer'):\n",
    "#                 self.no_nan_w = tf.get_variable('no_nan_w', shape=[self.conti_fea_cnt, ],\n",
    "#                                                 initializer=self.no_nan_w if self.no_nan_w else tf.ones_initializer())\n",
    "#                 self.nan_w = tf.get_variable('nan_w', shape=[self.conti_fea_cnt, ],\n",
    "#                                                          initializer=self.nan_w if self.nan_w else tf.zeros_initializer())\n",
    "#                 mv_conti_vec = tf.multiply(self.conti_vec, self.no_nan_w)\n",
    "#                 conti_zero_flag = tf.cast(tf.equal(mv_conti_vec, 0), tf.float32)\n",
    "#                 mv_conti_vec += tf.multiply(conti_zero_flag, tf.reshape(self.nan_w, [-1, self.nan_w.shape[0]]))\n",
    "\n",
    "            self.input_vecs.append(mv_conti_vec)\n",
    "            self.fm_fea_size += self.conti_fea_cnt\n",
    "\n",
    "            # 准备输入-----------------------------------------------------------------------------------------------------\n",
    "            fm_fea = tf.concat(self.input_vecs, axis=-1)\n",
    "\n",
    "            self.feat_index = [i for i in range(self.fm_fea_size)]\n",
    "            if self.fm_embedding_w is not None:\n",
    "                self.fea_embedding = tf.Variable(self.fm_embedding_w, name='fea_embedding', dtype=tf.float32)\n",
    "            else:\n",
    "                self.fea_embedding = tf.get_variable('fea_embedding', shape=[self.fm_fea_size, self.embedding_size],\n",
    "                                                     initializer=tf.keras.initializers.he_normal(), dtype=tf.float32)\n",
    "            # FM一阶部分权重\n",
    "            self.feature_bias = tf.get_variable('fea_bias', shape=[self.fm_fea_size, 1],\n",
    "                                                initializer=tf.keras.initializers.he_normal(), dtype=tf.float32)\n",
    "            # attention部分权重\n",
    "            self.attention_h = tf.Variable(np.random.normal(loc=0, scale=1, size=[self.att_layer,]), \n",
    "                                           dtype=np.float32, name='attention_h')\n",
    "            self.attention_p = tf.Variable(np.ones([self.embedding_size, ], dtype=np.float32), \n",
    "                                           dtype=tf.float32, name='attention_p')\n",
    "            # cross部分权重\n",
    "            self.cross_w = [tf.get_variable(name='cross_weight_%d' % i, shape=[self.fm_fea_size, 1],\n",
    "                                            initializer=tf.keras.initializers.he_normal(), dtype=tf.float32) for i in\n",
    "                            range(self.cross_layer_num)]\n",
    "            self.cross_b = [tf.get_variable(name='cross_bias_%d' % i, shape=[self.fm_fea_size, 1],\n",
    "                                            initializer=tf.keras.initializers.he_normal(), dtype=tf.float32) for i in\n",
    "                            range(self.cross_layer_num)]\n",
    "\n",
    "            # 构造输入\n",
    "            # 第二层embedding：潜在隐变量\n",
    "            embeddings = tf.nn.embedding_lookup(self.fea_embedding, self.feat_index)  # None * F * K\n",
    "            feat_value = tf.reshape(fm_fea, shape=[-1, self.fm_fea_size, 1])\n",
    "            embeddings = tf.multiply(embeddings, feat_value)  # None * F * K\n",
    "#             print(embeddings)\n",
    "#             embeddings = tf.Print(embeddings, [embeddings], message='Debug:', summarize=30)\n",
    "\n",
    "            # 搭建网络-----------------------------------------------------------------------------------------------------\n",
    "            # FM部分\n",
    "            with tf.variable_scope('FM-part'):\n",
    "                # first order term:输入为原始sparse features\n",
    "                y_first_order = tf.nn.embedding_lookup(self.feature_bias, self.feat_index)  # None * F * 1\n",
    "                y_first_order = tf.reduce_sum(tf.multiply(y_first_order, feat_value), 2)  # None * F(对1、2维求和都可以)\n",
    "                y_first_order = tf.nn.dropout(y_first_order, self.fm_drop_outs[0])  # None * F\n",
    "                # second order term:输入为dense embedding\n",
    "                summed_features_emb = tf.reduce_sum(embeddings, 1)  # None * K\n",
    "                summed_features_emb_square = tf.square(summed_features_emb)  # None * K\n",
    "                squared_features_emb = tf.square(embeddings)\n",
    "                squared_sum_features_emb = tf.reduce_sum(squared_features_emb, 1)  # None * K\n",
    "                y_second_order = 0.5 * tf.subtract(summed_features_emb_square, squared_sum_features_emb)  # None * K\n",
    "                y_second_order = tf.nn.dropout(y_second_order, self.fm_drop_outs[1])  # None * K\n",
    "#                 # second order term:加入attention\n",
    "#                 # Pair-wise Interation Layer\n",
    "#                 element_wise_product = None\n",
    "#                 for i in range(0, self.fm_fea_size):\n",
    "#                     if element_wise_product is None:\n",
    "#                         element_wise_product = tf.multiply(tf.gather(embeddings, [i], axis=1), \n",
    "#                                                            embeddings[:, i+1:self.fm_fea_size, :])\n",
    "#                     else:\n",
    "#                         element_wise_product = tf.concat([element_wise_product,\n",
    "#                                                          tf.multiply(tf.gather(embeddings, [i], axis=1), \n",
    "#                                                                      embeddings[:, i+1:self.fm_fea_size, :])],\n",
    "#                                                          axis=1) # None * (F*(F-1))/2 * K\n",
    "#                 # Attention-based Pooling Layer\n",
    "#                 attention_mul = tf.layers.Dense(self.att_layer)(element_wise_product)\n",
    "#                 attention_exp = tf.exp(tf.reduce_sum(tf.multiply(self.attention_h, tf.nn.relu(attention_mul)),\n",
    "#                                                      2, keep_dims=True))  # None * (H*(H-1)) * 1\n",
    "#                 attention_sum = tf.reduce_sum(attention_exp, 1, keep_dims=True)  # None * 1 * 1\n",
    "#                 attention_out = tf.div(attention_exp, attention_sum, name='attention_out')  #  None * (H*(H-1)) * 1\n",
    "#                 y_second_order = tf.reduce_sum(tf.multiply(attention_out, element_wise_product), 1, name='afm')  # None * K\n",
    "#                 y_second_order= tf.multiply(y_second_order, self.attention_p)  # None * K\n",
    "#                 y_second_order = tf.nn.dropout(y_second_order, self.fm_drop_outs[1])  # None * K\n",
    "    \n",
    "            # Cross Layer部分\n",
    "            with tf.variable_scope('Cross-part'):\n",
    "                x_0 = feat_value\n",
    "                x_l = x_0\n",
    "                for l in range(self.cross_layer_num):\n",
    "                    x_l = tf.tensordot(tf.matmul(x_0, x_l, transpose_b=True), self.cross_w[l], 1) + self.cross_b[\n",
    "                        l] + x_l\n",
    "                cross_output = tf.reshape(x_l, shape=[-1, self.fm_fea_size])\n",
    "\n",
    "            # DNN部分\n",
    "            with tf.variable_scope('Deep-part'):\n",
    "                y_deep = tf.reshape(embeddings, shape=[-1, self.fm_fea_size * self.embedding_size])  # None*(F*K)\n",
    "                y_deep = MLP(y_deep, self.dnn_layers)\n",
    "\n",
    "                # 合并\n",
    "            print('y_deep:{}, cross_output:{}, y_first_order:{}, y_second_order:{}'\n",
    "                  .format(y_deep, cross_output, y_first_order, y_second_order))\n",
    "#             last_input = tf.concat([y_deep], axis=-1) # DNN\n",
    "#             last_input = tf.concat([y_first_order, y_second_order], axis=-1) # FM\n",
    "#             last_input = tf.concat([y_deep, y_first_order, y_second_order], axis=-1) # DeepFM\n",
    "            last_input = tf.concat([y_deep, cross_output], axis=-1) # DCN\n",
    "#             last_input = tf.concat([y_deep, y_first_order, y_second_order, cross_output], axis=-1)  # DCFN\n",
    "\n",
    "            self.y_pre = tf.layers.Dense(1, activation=tf.nn.sigmoid, kernel_initializer=tf.keras.initializers.he_normal())(\n",
    "                last_input)  # 二分类\n",
    "#             self.y_pre = tf.layers.Dense(5, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_normal())(last_input) # 多分类\n",
    "\n",
    "            # 损失函数(二分类交叉熵等同于logloss)\n",
    "            self.loss = tf.losses.log_loss(self.label, self.y_pre)  # 二分类\n",
    "#             self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(tf.cast(self.label, tf.int32), 5), logits=self.y_pre)) # 多分类\n",
    "\n",
    "            # 优化方法\n",
    "            self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        self.saver.save(self.sess, model_path)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.saver.restore(self.sess, model_path)\n",
    "\n",
    "    def shuffle_csr_and_list(self, my_array, rng_state):\n",
    "        np.random.set_state(rng_state)\n",
    "        if type(my_array) == csr_matrix:\n",
    "            index = np.arange(np.shape(my_array)[0])\n",
    "            np.random.shuffle(index)\n",
    "            print('shuffle csr_matrix ' + str(my_array.shape))\n",
    "            return my_array[index, :]\n",
    "        else:\n",
    "            np.random.shuffle(my_array)\n",
    "            return my_array\n",
    "\n",
    "    def shuffle(self, cate_feas, conti_feas, labels):\n",
    "        rng_state = np.random.get_state()\n",
    "        cate_feas = self.shuffle_csr_and_list(cate_feas, rng_state)\n",
    "        conti_feas = self.shuffle_csr_and_list(conti_feas, rng_state)\n",
    "        labels = self.shuffle_csr_and_list(labels, rng_state)\n",
    "        return cate_feas, conti_feas, labels\n",
    "\n",
    "    def get_feed_dict(self, cate_feas, conti_feas, labels=None):\n",
    "        feed_dict = {\n",
    "            self.conti_vec: conti_feas,\n",
    "            self.cate_indexs: cate_feas.todense(),\n",
    "        }\n",
    "        if labels is not None:\n",
    "            feed_dict[self.label] = labels\n",
    "        return feed_dict\n",
    "\n",
    "    def gene_data(self, cate_feas, conti_feas, labels, bs, shuffle=False):\n",
    "        if shuffle:\n",
    "            cate_feas, conti_feas, labels = self.shuffle(cate_feas, conti_feas, labels)\n",
    "        bm = math.ceil(cate_feas.shape[0] / bs)\n",
    "        for j in range(bm):\n",
    "            a = cate_feas[j * bs:(j + 1) * bs]\n",
    "            b = conti_feas[j * bs:(j + 1) * bs]\n",
    "            c = labels[j * bs:(j + 1) * bs]\n",
    "            yield a, b, c\n",
    "\n",
    "    def gene_balance_data(self, cate_feas, conti_feas, labels, bs, shuffle=False):\n",
    "        pos_flag = np.array([l[0] == 1 for l in labels])\n",
    "        pos_indexing, neg_indexing = np.arange(len(labels))[pos_flag], np.arange(len(labels))[~pos_flag]\n",
    "        np.random.shuffle(neg_indexing)\n",
    "\n",
    "        bm = math.ceil(sum(~pos_flag) / bs)\n",
    "        for j in range(bm):\n",
    "            need_cnt = int(bs / 2)\n",
    "            crt_indexing = np.random.choice(pos_indexing, need_cnt).tolist() + neg_indexing[\n",
    "                                                                               j * need_cnt:(j + 1) * need_cnt].tolist()\n",
    "\n",
    "            a = cate_feas[crt_indexing, :]\n",
    "            b = np.take(conti_feas, crt_indexing, axis=0)\n",
    "            c = np.take(labels, crt_indexing, axis=0)\n",
    "            yield a, b, c\n",
    "\n",
    "    def fit(self, model_path, batch_size, epoch, cate_feas, conti_feas, labels, v_cate_feas, v_conti_feas, v_labels,\n",
    "            es=5):\n",
    "        print('start training ---------------------------------------------------')\n",
    "        logging.info('start train')\n",
    "        with self.graph.as_default():\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            best_auc = 0.0\n",
    "            no_num = 0\n",
    "            writer = tf.summary.FileWriter('./logs', self.sess.graph)\n",
    "            for i in range(epoch):\n",
    "                t1 = time()\n",
    "                epoch_losses = []\n",
    "                for cate_feas_batch, conti_feas_batch, labels_batch in self.gene_data(cate_feas, conti_feas,\n",
    "                                                                                      labels, batch_size,\n",
    "                                                                                      shuffle=False):\n",
    "                    feed = self.get_feed_dict(cate_feas_batch, conti_feas_batch, labels_batch)\n",
    "                    loss, _ = self.sess.run([self.loss, self.opt], feed_dict=feed)\n",
    "                    epoch_losses.append(loss)\n",
    "\n",
    "                v_loss, v_auc = self.eval(batch_size, v_cate_feas, v_conti_feas, v_labels)\n",
    "                t_loss = np.mean(np.array(epoch_losses))\n",
    "                logging.info('epoch: %s---train loss %.4f---valid loss: %.4f---valid auc: %.4f'\n",
    "                             % ((i + 1), t_loss, v_loss, v_auc))\n",
    "                print('epoch: %s---train loss %.4f---valid loss: %.4f---valid auc: %.4f [%.1f s]'\n",
    "                      % ((i + 1), t_loss, v_loss, v_auc, time() - t1))\n",
    "                if v_auc > best_auc:\n",
    "                    no_num = 0\n",
    "                    self.save_model(model_path)\n",
    "                    logging.info('---------- auc from %.4f to %.4f, saving model' % (best_auc, v_auc))\n",
    "                    print('---------- auc from %.4f to %.4f, saving model' % (best_auc, v_auc))\n",
    "                    best_auc = v_auc\n",
    "                else:\n",
    "                    no_num += 1\n",
    "                    self.lr = self.lr / 5\n",
    "                    if no_num >= es:\n",
    "                        break\n",
    "\n",
    "    def eval(self, batch_size, cate_feas, conti_feas, labels):\n",
    "        with self.graph.as_default():\n",
    "            y_pre = []\n",
    "            for cate_feas_batch, conti_feas_batch, label_batch in self.gene_data(cate_feas, conti_feas, labels,\n",
    "                                                                                 batch_size, shuffle=False):\n",
    "                feed = self.get_feed_dict(cate_feas_batch, conti_feas_batch, label_batch)\n",
    "                y_ = self.sess.run([self.y_pre], feed_dict=feed)[0]\n",
    "                y_pre += y_.tolist()\n",
    "            y_pre = np.array(y_pre)\n",
    "            #             print(y_pre)\n",
    "            y_pre = np.reshape(y_pre, (y_pre.shape[0],))\n",
    "            labels = np.reshape(labels, (labels.shape[0],))\n",
    "            loss = log_loss(labels, y_pre)\n",
    "            auc = roc_auc_score(labels, y_pre)\n",
    "            return loss, auc\n",
    "\n",
    "    def predict(self, cate_feas, conti_feas, batch_size):\n",
    "        def gd(cate_feas, conti_feas, bs):\n",
    "            bm = math.ceil(len(conti_feas) / bs)\n",
    "            for j in range(bm):\n",
    "                a = cate_feas[j * bs: (j + 1) * bs]\n",
    "                b = conti_feas[j * bs: (j + 1) * bs]\n",
    "                yield a, b\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            y_pre = []\n",
    "            for cate_feas_batch, conti_feas_batch in gd(cate_feas, conti_feas, batch_size):\n",
    "                feed = self.get_feed_dict(cate_feas_batch, conti_feas_batch)\n",
    "                y_ = self.sess.run([self.y_pre], feed_dict=feed)[0]\n",
    "                y_pre += y_.tolist()\n",
    "            y_pre = np.array(y_pre)\n",
    "            y_pre = np.reshape(y_pre, (y_pre.shape[0],))\n",
    "            return y_pre\n",
    "\n",
    "    def embedding_weights(self):\n",
    "        cate_embeddings, fea_embedding = self.sess.run([self.cate_embeddings, self.fea_embedding])\n",
    "        return cate_embeddings, fea_embedding\n",
    "\n",
    "    def miss_value_layer_w(self):\n",
    "        nan_embeddings, no_nan_embedding = self.sess.run([self.nan_w, self.no_nan_w])\n",
    "        return nan_embeddings, no_nan_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from py2ifttt import IFTTT\n",
    "\n",
    "# # 这里要填你之前的url后面的那段英文字母，然后 event_name 名字也要和你之前的一致\n",
    "# ifttt = IFTTT('your key str', 'event_name')\n",
    "# # 执行这句话，你就会收到推送\n",
    "# ifttt.notify('value1', 'value2', 'value3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                               title                        genres\n",
       "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4         5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6040, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id gender  age  occupation zip_code\n",
       "0        1      F    1          10    48067\n",
       "1        2      M   56          16    70072\n",
       "2        3      M   25          15    55117\n",
       "3        4      M   45           7    02460\n",
       "4        5      M   25          20    55455"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000209, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp\n",
       "0        1      1193       5       978300760\n",
       "1        1       661       3       978302109\n",
       "2        1       914       3       978301968\n",
       "3        1      3408       4       978300275\n",
       "4        1      2355       5       978824291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_cols = ['movie_id', 'title', 'genres']\n",
    "u_cols = ['user_id', 'gender', 'age', 'occupation', 'zip_code']\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "\n",
    "movies = pd.read_csv('./dataset/movieLens/movies.dat', sep='::', names=m_cols, encoding='latin-1')\n",
    "users = pd.read_csv('./dataset/movieLens/users.dat', sep='::', names=u_cols, encoding='latin-1')\n",
    "ratings = pd.read_csv('./dataset/movieLens/ratings.dat', sep='::', names=r_cols, encoding='latin-1')\n",
    "\n",
    "movies.shape\n",
    "movies.head()\n",
    "users.shape\n",
    "users.head()\n",
    "ratings.shape\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_genres(x):\n",
    "    return x.split('|')\n",
    "\n",
    "movies['split_genres'] = movies.genres.apply(split_genres)\n",
    "movies['len_genres'] = movies.split_genres.apply(lambda x:len(x))\n",
    "\n",
    "for i in range(6):\n",
    "    movies['genres' + str(i)] = movies.split_genres.apply(lambda x:x.pop() if len(x)>0 else None)\n",
    "movies['split_genres'] = movies.genres.apply(split_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_genres = list(movies.genres0) + list(movies.genres1) + list(movies.genres2) + list(movies.genres3) + list(movies.genres4) + list(movies.genres5)\n",
    "movie_genres = list(set(movie_genres))\n",
    "movie_genres.remove(None)\n",
    "len(movie_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_hot(x):\n",
    "    tmp = np.zeros(len(movie_genres))\n",
    "    for i, genre in enumerate(movie_genres):\n",
    "        if genre in x:\n",
    "            tmp[i] = 1\n",
    "    return list(map(int, tmp))\n",
    "\n",
    "movies['genres_multi_hot'] = movies.split_genres.apply(multi_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(movies.genres_multi_hot[0])):\n",
    "    movies['genres_multi_hot_' + str(i)] = movies.genres_multi_hot.apply(lambda x:x.pop() if len(x)>0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>len_genres</th>\n",
       "      <th>genres_multi_hot_0</th>\n",
       "      <th>genres_multi_hot_1</th>\n",
       "      <th>genres_multi_hot_2</th>\n",
       "      <th>genres_multi_hot_3</th>\n",
       "      <th>genres_multi_hot_4</th>\n",
       "      <th>genres_multi_hot_5</th>\n",
       "      <th>genres_multi_hot_6</th>\n",
       "      <th>genres_multi_hot_7</th>\n",
       "      <th>genres_multi_hot_8</th>\n",
       "      <th>genres_multi_hot_9</th>\n",
       "      <th>genres_multi_hot_10</th>\n",
       "      <th>genres_multi_hot_11</th>\n",
       "      <th>genres_multi_hot_12</th>\n",
       "      <th>genres_multi_hot_13</th>\n",
       "      <th>genres_multi_hot_14</th>\n",
       "      <th>genres_multi_hot_15</th>\n",
       "      <th>genres_multi_hot_16</th>\n",
       "      <th>genres_multi_hot_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id  len_genres  genres_multi_hot_0  genres_multi_hot_1  \\\n",
       "0         1           3                   0                   0   \n",
       "1         2           3                   0                   1   \n",
       "2         3           2                   0                   0   \n",
       "3         4           2                   0                   0   \n",
       "4         5           1                   0                   0   \n",
       "\n",
       "   genres_multi_hot_2  genres_multi_hot_3  genres_multi_hot_4  \\\n",
       "0                   0                   1                   1   \n",
       "1                   0                   0                   1   \n",
       "2                   0                   1                   0   \n",
       "3                   0                   1                   0   \n",
       "4                   0                   1                   0   \n",
       "\n",
       "   genres_multi_hot_5  genres_multi_hot_6  genres_multi_hot_7  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   genres_multi_hot_8  genres_multi_hot_9  genres_multi_hot_10  \\\n",
       "0                   0                   0                    0   \n",
       "1                   0                   1                    0   \n",
       "2                   0                   0                    0   \n",
       "3                   0                   0                    0   \n",
       "4                   0                   0                    0   \n",
       "\n",
       "   genres_multi_hot_11  genres_multi_hot_12  genres_multi_hot_13  \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   genres_multi_hot_14  genres_multi_hot_15  genres_multi_hot_16  \\\n",
       "0                    0                    1                    0   \n",
       "1                    0                    0                    0   \n",
       "2                    0                    0                    0   \n",
       "3                    1                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   genres_multi_hot_17  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    1  \n",
       "3                    0  \n",
       "4                    0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movies = movies[['movie_id', 'title', 'genres', 'len_genres', 'genres_multi_hot']]\n",
    "movies = movies.drop(['title','genres','split_genres','genres0','genres1','genres2','genres3','genres4','genres5','genres_multi_hot'], axis=1)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_feature(values):\n",
    "    uniq = values.unique()\n",
    "    mapping = dict(zip(uniq,range(1,len(uniq) + 1)))\n",
    "    return values.map(mapping)\n",
    "\n",
    "# movies.title = encode_feature(movies.title)\n",
    "users.gender = encode_feature(users.gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = ratings[['user_id', 'movie_id', 'rating']].merge(movies, on=['movie_id'], how='left')\n",
    "df = df.merge(users[['user_id', 'gender', 'age', 'occupation']], on=['user_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in ['gender', 'age', 'occupation']:\n",
    "    df = pd.concat([df, pd.get_dummies(df[i], prefix=i)], axis=1)\n",
    "    df = df.drop([i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating\n",
       "1     56174\n",
       "2    107557\n",
       "3    261197\n",
       "4    348971\n",
       "5    226310\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['rating']).user_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del movies\n",
    "del users\n",
    "del ratings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['rating_2'] = df.rating.apply(lambda x:0 if x<4 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 53)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>len_genres</th>\n",
       "      <th>genres_multi_hot_0</th>\n",
       "      <th>genres_multi_hot_1</th>\n",
       "      <th>genres_multi_hot_2</th>\n",
       "      <th>genres_multi_hot_3</th>\n",
       "      <th>genres_multi_hot_4</th>\n",
       "      <th>genres_multi_hot_5</th>\n",
       "      <th>genres_multi_hot_6</th>\n",
       "      <th>genres_multi_hot_7</th>\n",
       "      <th>genres_multi_hot_8</th>\n",
       "      <th>genres_multi_hot_9</th>\n",
       "      <th>genres_multi_hot_10</th>\n",
       "      <th>genres_multi_hot_11</th>\n",
       "      <th>genres_multi_hot_12</th>\n",
       "      <th>genres_multi_hot_13</th>\n",
       "      <th>genres_multi_hot_14</th>\n",
       "      <th>genres_multi_hot_15</th>\n",
       "      <th>genres_multi_hot_16</th>\n",
       "      <th>genres_multi_hot_17</th>\n",
       "      <th>gender_1</th>\n",
       "      <th>gender_2</th>\n",
       "      <th>age_1</th>\n",
       "      <th>age_18</th>\n",
       "      <th>age_25</th>\n",
       "      <th>age_35</th>\n",
       "      <th>age_45</th>\n",
       "      <th>age_50</th>\n",
       "      <th>age_56</th>\n",
       "      <th>occupation_0</th>\n",
       "      <th>occupation_1</th>\n",
       "      <th>occupation_2</th>\n",
       "      <th>occupation_3</th>\n",
       "      <th>occupation_4</th>\n",
       "      <th>occupation_5</th>\n",
       "      <th>occupation_6</th>\n",
       "      <th>occupation_7</th>\n",
       "      <th>occupation_8</th>\n",
       "      <th>occupation_9</th>\n",
       "      <th>occupation_10</th>\n",
       "      <th>occupation_11</th>\n",
       "      <th>occupation_12</th>\n",
       "      <th>occupation_13</th>\n",
       "      <th>occupation_14</th>\n",
       "      <th>occupation_15</th>\n",
       "      <th>occupation_16</th>\n",
       "      <th>occupation_17</th>\n",
       "      <th>occupation_18</th>\n",
       "      <th>occupation_19</th>\n",
       "      <th>occupation_20</th>\n",
       "      <th>rating_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  len_genres  genres_multi_hot_0  \\\n",
       "0        1      1193       5           1                   0   \n",
       "1        1       661       3           3                   0   \n",
       "2        1       914       3           2                   0   \n",
       "3        1      3408       4           1                   0   \n",
       "4        1      2355       5           3                   0   \n",
       "\n",
       "   genres_multi_hot_1  genres_multi_hot_2  genres_multi_hot_3  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   1   \n",
       "\n",
       "   genres_multi_hot_4  genres_multi_hot_5  genres_multi_hot_6  \\\n",
       "0                   0                   0                   0   \n",
       "1                   1                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   1                   0                   0   \n",
       "\n",
       "   genres_multi_hot_7  genres_multi_hot_8  genres_multi_hot_9  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   genres_multi_hot_10  genres_multi_hot_11  genres_multi_hot_12  \\\n",
       "0                    0                    0                    0   \n",
       "1                    0                    0                    1   \n",
       "2                    0                    0                    1   \n",
       "3                    0                    0                    0   \n",
       "4                    0                    0                    0   \n",
       "\n",
       "   genres_multi_hot_13  genres_multi_hot_14  genres_multi_hot_15  \\\n",
       "0                    0                    1                    0   \n",
       "1                    0                    0                    1   \n",
       "2                    0                    0                    0   \n",
       "3                    0                    1                    0   \n",
       "4                    0                    0                    1   \n",
       "\n",
       "   genres_multi_hot_16  genres_multi_hot_17  gender_1  gender_2  age_1  \\\n",
       "0                    0                    0         1         0      1   \n",
       "1                    0                    0         1         0      1   \n",
       "2                    0                    1         1         0      1   \n",
       "3                    0                    0         1         0      1   \n",
       "4                    0                    0         1         0      1   \n",
       "\n",
       "   age_18  age_25  age_35  age_45  age_50  age_56  occupation_0  occupation_1  \\\n",
       "0       0       0       0       0       0       0             0             0   \n",
       "1       0       0       0       0       0       0             0             0   \n",
       "2       0       0       0       0       0       0             0             0   \n",
       "3       0       0       0       0       0       0             0             0   \n",
       "4       0       0       0       0       0       0             0             0   \n",
       "\n",
       "   occupation_2  occupation_3  occupation_4  occupation_5  occupation_6  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   occupation_7  occupation_8  occupation_9  occupation_10  occupation_11  \\\n",
       "0             0             0             0              1              0   \n",
       "1             0             0             0              1              0   \n",
       "2             0             0             0              1              0   \n",
       "3             0             0             0              1              0   \n",
       "4             0             0             0              1              0   \n",
       "\n",
       "   occupation_12  occupation_13  occupation_14  occupation_15  occupation_16  \\\n",
       "0              0              0              0              0              0   \n",
       "1              0              0              0              0              0   \n",
       "2              0              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   occupation_17  occupation_18  occupation_19  occupation_20  rating_2  \n",
       "0              0              0              0              0         1  \n",
       "1              0              0              0              0         0  \n",
       "2              0              0              0              0         0  \n",
       "3              0              0              0              0         1  \n",
       "4              0              0              0              0         1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.astype('int16')\n",
    "\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800167, 51) (200042, 51)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['rating', 'rating_2'], axis=1), df.rating_2, test_size=0.2, random_state =1024)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[a-z]_[0-9]+')\n",
    "conti_feas = ['len_genres']\n",
    "cate_feas = ['user_id', 'movie_id', 'genres_multi_hot', 'gender', 'age', 'occupation']\n",
    "cate_long_feas = ['user_id', 'movie_id']\n",
    "\n",
    "cate_embedding_uni_cnt = {'user_id':6040, 'movie_id':3952, 'genres_multi_hot':18, 'gender':2, 'age':7, 'occupation':21}\n",
    "cate_embedding_uni_cnt_list = [cate_embedding_uni_cnt[i] for i in cate_feas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_ori_cate_feas(data_list, cate_feas):\n",
    "#     col_num = sum([cate_embedding_uni_cnt[i] for i in cate_feas])\n",
    "    format_data_list = []\n",
    "    offset = 0\n",
    "    for ii, i in enumerate(cate_feas):\n",
    "        rows, cols, data = [], [], []\n",
    "        tmp_len = max(data_list[i])\n",
    "        for j in range(data_list.shape[0]):\n",
    "            if(df[i][j] != 0):\n",
    "                rows += [j]\n",
    "                cols += [data_list[i][j]-1]\n",
    "#                 cols += [df[i][j]-1+offset]\n",
    "                data += [1]\n",
    "        if ii == 0:\n",
    "            tmp_csr = csr_matrix((data, (rows, cols)), shape=(data_list.shape[0], tmp_len))\n",
    "        else:\n",
    "            tmp_csr = hstack([tmp_csr, csr_matrix((data, (rows, cols)), shape=(data_list.shape[0], tmp_len))])\n",
    "#         offset += tmp_len\n",
    "#         format_data_list.append(csr_matrix((data, (rows, cols)), shape=(data_list.shape[0], col_num)))\n",
    "    return tmp_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "conti_cols = conti_feas\n",
    "data = pd.concat([X_train[conti_cols], X_test[conti_cols]])\n",
    "for fea in conti_cols:\n",
    "    scaler_val = data[fea][~data[fea].isnull()].values\n",
    "    scaler = StandardScaler().fit(scaler_val.reshape((len(scaler_val), 1)))\n",
    "    data[fea].fillna(scaler.mean_[0], inplace=True)\n",
    "    data[fea] = scaler.transform(data[fea].values.reshape((len(data), 1))).reshape((len(data),)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading conti data...\n",
      "train conti feas shape: (800167, 1), val conti feas shape: (200042, 1)\n",
      "loading ori cate data...\n",
      "train cate shape:(800167, 10040), val cate shape:(200042, 10040)\n",
      "training...\n",
      "y_deep:Tensor(\"Deep-part/LeakyRelu_2/Maximum:0\", shape=(?, 128), dtype=float32), cross_output:Tensor(\"Cross-part/Reshape:0\", shape=(?, 46), dtype=float32), y_first_order:Tensor(\"FM-part/dropout/mul:0\", shape=(?, 46), dtype=float32), y_second_order:Tensor(\"FM-part/dropout_1/mul:0\", shape=(?, 8), dtype=float32)\n",
      "start training ---------------------------------------------------\n",
      "epoch: 1---train loss 0.5577---valid loss: 0.5395---valid auc: 0.7919 [326.8 s]\n",
      "---------- auc from 0.0000 to 0.7919, saving model\n",
      "epoch: 2---train loss 0.5313---valid loss: 0.5347---valid auc: 0.7972 [227.4 s]\n",
      "---------- auc from 0.7919 to 0.7972, saving model\n",
      "epoch: 3---train loss 0.5172---valid loss: 0.5291---valid auc: 0.8037 [413.5 s]\n",
      "---------- auc from 0.7972 to 0.8037, saving model\n",
      "epoch: 4---train loss 0.5010---valid loss: 0.5279---valid auc: 0.8061 [129.2 s]\n",
      "---------- auc from 0.8037 to 0.8061, saving model\n",
      "epoch: 5---train loss 0.4877---valid loss: 0.5321---valid auc: 0.8047 [131.1 s]\n",
      "epoch: 6---train loss 0.4763---valid loss: 0.5402---valid auc: 0.8021 [319.9 s]\n"
     ]
    }
   ],
   "source": [
    "print('loading conti data...')\n",
    "# train_conti_feas, val_conti_feas = X_train[[]].as_matrix(), X_test[[]].as_matrix()\n",
    "# train_conti_feas, val_conti_feas = data[:X_train.shape[0]].as_matrix(), data[X_train.shape[0]:].as_matrix()\n",
    "print('train conti feas shape: {}, val conti feas shape: {}'.format(np.shape(train_conti_feas),\n",
    "                                                                    np.shape(val_conti_feas)))\n",
    "\n",
    "print('loading ori cate data...')\n",
    "# train_cate_csr = build_ori_cate_feas(X_train, cate_long_feas)\n",
    "# train_cate_csr = hstack([train_cate_csr, csr_matrix(X_train.iloc[:, -sum([cate_embedding_uni_cnt[i] for i in list(set(cate_feas)-set(cate_long_feas))]):].as_matrix())])\n",
    "# train_cate_csr = csr_matrix(train_cate_csr)\n",
    "# val_cate_csr = build_ori_cate_feas(X_test, cate_long_feas)\n",
    "# val_cate_csr = hstack([val_cate_csr, csr_matrix(X_test.iloc[:, -sum([cate_embedding_uni_cnt[i] for i in list(set(cate_feas)-set(cate_long_feas))]):].as_matrix())])\n",
    "# val_cate_csr = csr_matrix(val_cate_csr)\n",
    "print('train cate shape:{}, val cate shape:{}'.format(train_cate_csr.shape, val_cate_csr.shape))\n",
    "\n",
    "print('training...')\n",
    "model_name = 'movieLens'\n",
    "cate_embedding_w_list, fm_embedding_w, no_nan_w, nan_w = None, None, None, None\n",
    "\n",
    "dcfn_params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'embedding_size': 8,\n",
    "    'dnn_layers': [2048, 512, 128],\n",
    "    'att_layer': 8,\n",
    "    'cross_layer_num': 1,\n",
    "    'conti_fea_cnt': train_conti_feas.shape[1],\n",
    "    'cate_embedding_uni_cnt_list': cate_embedding_uni_cnt_list,\n",
    "    'cate_embedding_w_list': cate_embedding_w_list,\n",
    "    'fm_embedding_w': fm_embedding_w,\n",
    "    'no_nan_w': no_nan_w,\n",
    "    'nan_w': nan_w,\n",
    "    'fm_drop_outs': [0.5, 0.5]\n",
    "}\n",
    "model = DCFN(**dcfn_params)\n",
    "\n",
    "fit_params = {\n",
    "    'model_path': './model/nn/dcfm_%s.ckpt' % model_name,\n",
    "    'batch_size': 1024,\n",
    "    'epoch': 100,\n",
    "    'cate_feas': train_cate_csr,\n",
    "    'conti_feas': train_conti_feas,\n",
    "    'labels': y_train.values.reshape(-1, 1),\n",
    "    'v_cate_feas': val_cate_csr,\n",
    "    'v_conti_feas': val_conti_feas,\n",
    "    'v_labels': y_test.values.reshape(-1, 1),\n",
    "    'es': 2\n",
    "}\n",
    "model.fit(**fit_params)\n",
    "# cate_w, fm_em_w = model.embedding_weights()\n",
    "# nan_w, no_nan_w = model.miss_value_layer_w()\n",
    "# np.array(cate_w).dump('./model/nn/%s_cate_w.np' % model_name)\n",
    "# np.array(fm_em_w).dump('./model/nn/%s_fm_em_w.np' % model_name)\n",
    "# np.array(nan_w).dump('./model/nn/%s_nan_w.np' % model_name)\n",
    "# np.array(no_nan_w).dump('./model/nn/%s_no_nan_w.np' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DCFN\n",
    "0.001\n",
    "epoch: 4---train loss 0.4972---valid loss: 0.5265---valid auc: 0.8074 [93.0 s]\n",
    "---------- auc from 0.8059 to 0.8074, saving model\n",
    "0.0005\n",
    "epoch: 7---train loss 0.4818---valid loss: 0.5312---valid auc: 0.8073 [73.3 s]\n",
    "---------- auc from 0.8068 to 0.8073, saving model\n",
    "\n",
    "0.001+tf.keras.initializers.he_normal\n",
    "epoch: 4---train loss 0.4977---valid loss: 0.5250---valid auc: 0.8081 [92.6 s]\n",
    "---------- auc from 0.8073 to 0.8081, saving model\n",
    "\n",
    "0.001+tf.keras.initializers.he_normal+tf.nn.leaky_relu\n",
    "epoch: 5---train loss 0.4876---valid loss: 0.5266---valid auc: 0.8089 [100.8 s]\n",
    "---------- auc from 0.8078 to 0.8089, saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FM\n",
    "epoch: 15---train loss 0.5151---valid loss: 0.5369---valid auc: 0.7981 [108.9 s]\n",
    "---------- auc from 0.7974 to 0.7981, saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep-FM\n",
    "[512, 256]\n",
    "epoch: 7---train loss 0.4737---valid loss: 0.5389---valid auc: 0.8041 [75.3 s]\n",
    "---------- auc from 0.8040 to 0.8041, saving model\n",
    "\n",
    "[1024, 512, 256, 128]\n",
    "epoch: 4---train loss 0.5041---valid loss: 0.5278---valid auc: 0.8061 [75.7 s]\n",
    "---------- auc from 0.8035 to 0.8061, saving model\n",
    "\n",
    "[2048, 512, 128]\n",
    "epoch: 4---train loss 0.5003---valid loss: 0.5262---valid auc: 0.8069 [128.1 s]\n",
    "---------- auc from 0.8054 to 0.8069, saving model\n",
    "\n",
    "[256, 128]\n",
    "epoch: 4---train loss 0.5040---valid loss: 0.5278---valid auc: 0.8052 [80.3 s]\n",
    "---------- auc from 0.8042 to 0.8052, saving model\n",
    "\n",
    "[256, 128, 32]\n",
    "epoch: 4---train loss 0.5057---valid loss: 0.5292---valid auc: 0.8048 [175.8 s]\n",
    "---------- auc from 0.8027 to 0.8048, saving model\n",
    "\n",
    "[2048, 512, 128, 32]\n",
    "epoch: 4---train loss 0.5023---valid loss: 0.5276---valid auc: 0.8057 [268.1 s]\n",
    "---------- auc from 0.8042 to 0.8057, saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-layer\n",
    "1\n",
    "epoch: 4---train loss 0.5054---valid loss: 0.5288---valid auc: 0.8053 [107.3 s]\n",
    "---------- auc from 0.8024 to 0.8053, saving model\n",
    "\n",
    "2\n",
    "epoch: 6---train loss 0.4865---valid loss: 0.5329---valid auc: 0.8051 [137.2 s]\n",
    "---------- auc from 0.8049 to 0.8051, saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding-size\n",
    "4\n",
    "epoch: 2---train loss 0.5316---valid loss: 0.5344---valid auc: 0.7971 [244.2 s]\n",
    "---------- auc from 0.7919 to 0.7971, saving model\n",
    "\n",
    "16\n",
    "epoch: 4---train loss 0.4992---valid loss: 0.5278---valid auc: 0.8058 [311.2 s]\n",
    "---------- auc from 0.8043 to 0.8058, saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep-layer\n",
    "[512, 512]\n",
    "epoch: 2---train loss 0.5311---valid loss: 0.5332---valid auc: 0.7976 [118.2 s]\n",
    "---------- auc from 0.7919 to 0.7976, saving model\n",
    "\n",
    "[64, 32]\n",
    "epoch: 2---train loss 0.5328---valid loss: 0.5361---valid auc: 0.7952 [60.0 s]\n",
    "---------- auc from 0.7908 to 0.7952, saving model\n",
    "\n",
    "[128, 64]\n",
    "epoch: 2---train loss 0.5323---valid loss: 0.5355---valid auc: 0.7956 [75.7 s]\n",
    "---------- auc from 0.7913 to 0.7956, saving model\n",
    "            \n",
    "[128, 128]\n",
    "epoch: 2---train loss 0.5293---valid loss: 0.5332---valid auc: 0.7980 [62.8 s]\n",
    "---------- auc from 0.7934 to 0.7980, saving model\n",
    "\n",
    "[256, 128]\n",
    "epoch: 2---train loss 0.5287---valid loss: 0.5320---valid auc: 0.7990 [67.3 s]\n",
    "---------- auc from 0.7931 to 0.7990, saving model\n",
    "\n",
    "[256, 128, 64]\n",
    "epoch: 2---train loss 0.5300---valid loss: 0.5332---valid auc: 0.7982 [70.5 s]\n",
    "---------- auc from 0.7923 to 0.7982, saving model\n",
    "\n",
    "[256, 128, 64, 32]\n",
    "epoch: 2---train loss 0.5326---valid loss: 0.5348---valid auc: 0.7963 [227.4 s]\n",
    "---------- auc from 0.7903 to 0.7963, saving model\n",
    "\n",
    "[512, 256]\n",
    "epoch: 2---train loss 0.5300---valid loss: 0.5332---valid auc: 0.7979 [67.0 s]\n",
    "---------- auc from 0.7920 to 0.7979, saving model\n",
    "\n",
    "[2048, 512, 128]\n",
    "epoch: 4---train loss 0.5024---valid loss: 0.5245---valid auc: 0.8083 [81.2 s]\n",
    "---------- auc from 0.8049 to 0.8083, saving model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
